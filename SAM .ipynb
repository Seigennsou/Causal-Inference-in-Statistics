{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "7Oeh5kch4Q1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "print(torch.__version__)\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.random import *\n",
        "# グラフの描画用\n",
        "import matplotlib.pyplot as plt\n",
        "# シグモイド関数をimport\n",
        "from scipy.special import expit"
      ],
      "metadata": {
        "id": "-uSrTVqlVBbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "metadata": {
        "id": "DIG67PEcVTaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "O62u2-uPVTeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Titanic Data/train.csv')"
      ],
      "metadata": {
        "id": "5rhrBU4WVaZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#不要な列・欠損が多い列を削除し、カテゴリを数値に変換\n",
        "df = df.drop('PassengerId',axis=1).drop('Name',axis=1).drop('Ticket',axis=1).drop('Cabin',axis=1)\n",
        "df = pd.get_dummies(df)"
      ],
      "metadata": {
        "id": "09rGvJd3Z5zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "6BnOVzNQaLNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "TkIptmnzaY7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "X8w3hmwbaabi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Age'] = df['Age'].fillna(df['Age'].mean())"
      ],
      "metadata": {
        "id": "CNkvr84pagrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "qZhGiFriakR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cdt"
      ],
      "metadata": {
        "id": "YEPzsvSTanx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorchから使用するものをimport\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SAMDiscriminator(nn.Module):\n",
        "    \"\"\"SAMのDiscriminatorのニューラルネットワーク\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nfeatures, dnh, hlayers):\n",
        "        super(SAMDiscriminator, self).__init__()\n",
        "\n",
        "        # ----------------------------------\n",
        "        # ネットワークの用意\n",
        "        # ----------------------------------\n",
        "        self.nfeatures = nfeatures  # 入力変数の数\n",
        "\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(nfeatures,dnh))\n",
        "        layers.append(nn.BatchNorm1d(dnh))\n",
        "        layers.append(nn.LeakyReLU(.2))\n",
        "\n",
        "        for i in range(hlayers-1):\n",
        "            layers.append(nn.Linear(dnh,dnh))\n",
        "            layers.append(nn.BatchNorm1d(dnh))\n",
        "            layers.append(nn.LeakyReLU(.2))\n",
        "\n",
        "        layers.append(nn.Linear(dnh,1))  # 最終出力\n",
        "\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "        # ----------------------------------\n",
        "        # maskの用意（対角成分のみ1で、他は0の行列）\n",
        "        # ----------------------------------\n",
        "        mask = torch.eye(nfeatures, nfeatures)  # 変数の数×変数の数の単位行列\n",
        "        self.register_buffer(\"mask\", mask.unsqueeze(0))  # 単位行列maskを保存しておく\n",
        "\n",
        "        # 注意：register_bufferはmodelのパラメータではないが、その後forwardで使う変数を登録するPyTorchのメソッドです\n",
        "        # self.変数名で、以降も使用可能になります\n",
        "        # https://pytorch.org/docs/stable/nn.html?highlight=register_buffer#torch.nn.Module.register_buffer\n",
        "\n",
        "    def forward(self, input, obs_data=None):\n",
        "        \"\"\"　順伝搬の計算\n",
        "        Args:\n",
        "            input (torch.Size([データ数, 観測変数の種類数])): 観測したデータ、もしくは生成されたデータ\n",
        "            obs_data (torch.Size([データ数, 観測変数の種類数])):観測したデータ\n",
        "        Returns:\n",
        "            torch.Tensor: 観測したデータか、それとも生成されたデータかの判定結果\n",
        "        \"\"\"\n",
        "\n",
        "        if obs_data is not None:\n",
        "          # 生成データを識別器に入力する場合\n",
        "            return [self.layers(i) for i in torch.unbind(obs_data.unsqueeze(1) * (1 - self.mask)\n",
        "                                                         + input.unsqueeze(1) * self.mask, 1)]\n",
        "            # 対角成分のみ生成したデータ、その他は観測データに\n",
        "            # データを各変数ごとに、生成したもの、その他観測したもので混ぜて、1変数ずつ生成したものを放り込む\n",
        "            # torch.unbind(x,1)はxの1次元目でテンソルをタプルに展開する\n",
        "            # minibatch数が2000、観測データの変数が6種類の場合、\n",
        "            # [2000,6]→[2000,6,6]→([2000,6], [2000,6], [2000,6], [2000,6], [2000,6], [2000,6])→([2000,1], [2000,1], [2000,1], [2000,1], [2000,1], [2000,1])\n",
        "            # returnは[torch.Size([2000, 1]),torch.Size([2000, 1]),torch.Size([2000, 1], torch.Size([2000, 1]),torch.Size([2000, 1]),torch.Size([2000, 1])]\n",
        "\n",
        "            # 注：生成した変数全種類を用いた判定はしない。\n",
        "            # すなわち、生成した変数1種類と、元の観測データたちをまとめて1つにし、それが観測結果か、生成結果を判定させる\n",
        "\n",
        "        else:\n",
        "            # 観測データを識別器に入力する場合\n",
        "\n",
        "            return self.layers(input)\n",
        "            # returnは[torch.Size([2000, 1])]\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"識別器Dの重みパラメータの初期化を実施\"\"\"\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'reset_parameters'):\n",
        "                layer.reset_parameters()"
      ],
      "metadata": {
        "id": "VoZ7xrcLa2gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cdt.utils.torch import ChannelBatchNorm1d, MatrixSampler, Linear3D\n",
        "\n",
        "\n",
        "class SAMGenerator(nn.Module):\n",
        "    \"\"\"SAMのGeneratorのニューラルネットワーク\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_shape, nh):\n",
        "        \"\"\"初期化\"\"\"\n",
        "        super(SAMGenerator, self).__init__()\n",
        "\n",
        "        # ----------------------------------\n",
        "        # 対角成分のみ0で、残りは1のmaskとなる変数skeletonを作成\n",
        "        # ※最後の行は、全部1です\n",
        "        # ----------------------------------\n",
        "        nb_vars = data_shape[1]  # 変数の数\n",
        "        skeleton = 1 - torch.eye(nb_vars + 1, nb_vars)\n",
        "\n",
        "        self.register_buffer('skeleton', skeleton)\n",
        "\n",
        "        # 注意：register_bufferはmodelのパラメータではないが、その後forwardで使う変数を登録するPyTorchのメソッドです\n",
        "        # self.変数名で、以降も使用可能になります\n",
        "        # https://pytorch.org/docs/stable/nn.html?highlight=register_buffer#torch.nn.Module.register_buffer\n",
        "\n",
        "        # ----------------------------------\n",
        "        # ネットワークの用意\n",
        "        # ----------------------------------\n",
        "        # 入力層（SAMの形での全結合層）　\n",
        "        self.input_layer = Linear3D((nb_vars,nb_vars + 1,nh))  # nhは中間層のニューロン数\n",
        "        # https://github.com/FenTechSolutions/CausalDiscoveryToolbox/blob/32200779ab9b63762be3a24a2147cff09ba2bb72/cdt/utils/torch.py#L289\n",
        "\n",
        "        # 中間層\n",
        "        layers = []\n",
        "        # 2次元を1次元に変換してバッチノーマライゼーションするモジュール\n",
        "        layers.append(ChannelBatchNorm1d(nb_vars, nh))\n",
        "        layers.append(nn.Tanh())\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "\n",
        "        # ChannelBatchNorm1d\n",
        "        # https://github.com/FenTechSolutions/CausalDiscoveryToolbox/blob/32200779ab9b63762be3a24a2147cff09ba2bb72/cdt/utils/torch.py#L130\n",
        "\n",
        "        # 出力層（再度、SAMの形での全結合層）\n",
        "        self.output_layer = Linear3D((nb_vars,nh,1))\n",
        "\n",
        "    def forward(self, data, noise, adj_matrix, drawn_neurons=None):\n",
        "        \"\"\"　順伝搬の計算\n",
        "        Args:\n",
        "            data (torch.Tensor): 観測データ\n",
        "            noise (torch.Tensor): データ生成用のノイズ\n",
        "            adj_matrix (torch.Tensor): 因果関係を示す因果構造マトリクスM\n",
        "            drawn_neurons (torch.Tensor): Linear3Dの複雑さを制御する複雑さマトリクスZ\n",
        "        Returns:\n",
        "            torch.Tensor: 生成されたデータ\n",
        "        \"\"\"\n",
        "\n",
        "        # 入力層\n",
        "        x = self.input_layer(data, noise, adj_matrix * self.skeleton)  # Linear3D\n",
        "\n",
        "        # 中間層（バッチノーマライゼーションとTanh）\n",
        "        x = self.layers(x)\n",
        "\n",
        "        # 出力層\n",
        "        output = self.output_layer(x, noise=None, adj_matrix=drawn_neurons)  # Linear3D\n",
        "\n",
        "        return output.squeeze(2)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        \"\"\"重みパラメータの初期化を実施\"\"\"\n",
        "\n",
        "        self.input_layer.reset_parameters()\n",
        "        self.output_layer.reset_parameters()\n",
        "\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'reset_parameters'):\n",
        "                layer.reset_parameters()"
      ],
      "metadata": {
        "id": "XFKOxcD2a80F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ネットワークを示す因果構造マトリクスMがDAG（有向非循環グラフ）になるように加える損失\n",
        "\n",
        "def notears_constr(adj_m, max_pow=None):\n",
        "    \"\"\"No Tears constraint for binary adjacency matrixes. \n",
        "    Args:\n",
        "        adj_m (array-like): Adjacency matrix of the graph\n",
        "        max_pow (int): maximum value to which the infinite sum is to be computed.\n",
        "           defaults to the shape of the adjacency_matrix\n",
        "    Returns:\n",
        "        np.ndarray or torch.Tensor: Scalar value of the loss with the type\n",
        "            depending on the input.\n",
        "    参考：https://github.com/FenTechSolutions/CausalDiscoveryToolbox/blob/32200779ab9b63762be3a24a2147cff09ba2bb72/cdt/utils/loss.py#L215\n",
        "    \"\"\"\n",
        "    m_exp = [adj_m]\n",
        "    if max_pow is None:\n",
        "        max_pow = adj_m.shape[1]\n",
        "    while(m_exp[-1].sum() > 0 and len(m_exp) < max_pow):\n",
        "        m_exp.append(m_exp[-1] @ adj_m/len(m_exp))\n",
        "\n",
        "    return sum([i.diag().sum() for idx, i in enumerate(m_exp)])\n",
        "    "
      ],
      "metadata": {
        "id": "tUdRgvyrbA-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import scale\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def run_SAM(in_data,lr_gen,lr_disc,lambda1,lambda2,hlayers,nh,dnh,train_epochs,test_epochs,device):\n",
        "    '''SAMの学習を実行する関数'''\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # 入力データの前処理\n",
        "    # ---------------------------------------------------\n",
        "    list_nodes = list(in_data.columns)  # 入力データの列名のリスト\n",
        "    data = scale(in_data[list_nodes].values)  # 入力データの正規化\n",
        "    nb_var = len(list_nodes)  # 入力データの数 = d\n",
        "    data = data.astype('float32')  # 入力データをfloat32型に\n",
        "    data = torch.from_numpy(data).to(device)  # 入力データをPyTorchのテンソルに\n",
        "    rows, cols = data.size()  # rowsはデータ数、colsは変数の数\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # DataLoaderの作成（バッチサイズは全データ）\n",
        "    # ---------------------------------------------------\n",
        "    batch_size = rows  # 入力データ全てを使用したミニバッチ学習とする\n",
        "    data_iterator = DataLoader(data, batch_size=batch_size,\n",
        "                               shuffle=True, drop_last=True)\n",
        "    # 注意：引数のdrop_lastはdataをbatch_sizeで取り出していったときに最後に余ったものは使用しない設定\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # 【Generator】ネットワークの生成とパラメータの初期化\n",
        "    # cols：入力変数の数、nhは中間ニューロンの数、hlayersは中間層の数\n",
        "    # neuron_samplerは、Functional gatesの変数zを学習するネットワーク\n",
        "    # graph_samplerは、Structual gatesの変数aを学習するネットワーク\n",
        "    # ---------------------------------------------------\n",
        "    sam = SAMGenerator((batch_size, cols), nh).to(device)  # 生成器G\n",
        "    graph_sampler = MatrixSampler(nb_var, mask=None, gumbel=False).to(\n",
        "        device)  # 因果構造マトリクスMを作るネットワーク\n",
        "    neuron_sampler = MatrixSampler((nh, nb_var), mask=False, gumbel=True).to(\n",
        "        device)  # 複雑さマトリクスZを作るネットワーク\n",
        "\n",
        "    # 注意：MatrixSamplerはGumbel-Softmaxを使用し、0か1を出力させるニューラルネットワーク\n",
        "    # SAMの著者らの実装モジュール、MatrixSamplerを使用\n",
        "    # https://github.com/FenTechSolutions/CausalDiscoveryToolbox/blob/32200779ab9b63762be3a24a2147cff09ba2bb72/cdt/utils/torch.py#L212\n",
        "\n",
        "    # 重みパラメータの初期化\n",
        "    sam.reset_parameters()\n",
        "    graph_sampler.weights.data.fill_(2)\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # 【Discriminator】ネットワークの生成とパラメータの初期化\n",
        "    # cols：入力変数の数、dnhは中間ニューロンの数、hlayersは中間層の数。\n",
        "    # ---------------------------------------------------\n",
        "    discriminator = SAMDiscriminator(cols, dnh, hlayers).to(device)\n",
        "    discriminator.reset_parameters()  # 重みパラメータの初期化\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # 最適化の設定\n",
        "    # ---------------------------------------------------\n",
        "    # 生成器\n",
        "\n",
        "    g_optimizer = optim.Adam(sam.parameters(), lr=lr_gen)\n",
        "    graph_optimizer = optim.Adam(graph_sampler.parameters(), lr=lr_gen)\n",
        "    neuron_optimizer = optim.Adam(neuron_sampler.parameters(), lr=lr_gen)\n",
        "\n",
        "    # 識別器\n",
        "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr_disc)\n",
        "\n",
        "    # 損失関数\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    # nn.BCEWithLogitsLoss()は、binary cross entropy with Logistic function\n",
        "    # https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss\n",
        "\n",
        "    # 損失関数のDAGに関する制約の設定パラメータ\n",
        "    dagstart = 0.5\n",
        "    dagpenalization_increase = 0.001*10\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # forward計算、および損失関数の計算に使用する変数を用意\n",
        "    # ---------------------------------------------------\n",
        "    _true = torch.ones(1).to(device)\n",
        "    _false = torch.zeros(1).to(device)\n",
        "\n",
        "    noise = torch.randn(batch_size, nb_var).to(device)  # 生成器Gで使用する生成ノイズ\n",
        "    noise_row = torch.ones(1, nb_var).to(device)\n",
        "\n",
        "    output = torch.zeros(nb_var, nb_var).to(device)  # 求まった隣接行列\n",
        "    output_loss = torch.zeros(1, 1).to(device)\n",
        "\n",
        "    # ---------------------------------------------------\n",
        "    # forwardの計算で、ネットワークを学習させる\n",
        "    # ---------------------------------------------------\n",
        "    pbar = tqdm(range(train_epochs + test_epochs),mininterval=1)  # 進捗（progressive bar）の表示\n",
        "\n",
        "    for epoch in pbar:\n",
        "        for i_batch, batch in enumerate(data_iterator):\n",
        "\n",
        "            # 最適化を初期化\n",
        "            g_optimizer.zero_grad()\n",
        "            graph_optimizer.zero_grad()\n",
        "            neuron_optimizer.zero_grad()\n",
        "            d_optimizer.zero_grad()\n",
        "\n",
        "            # 因果構造マトリクスM（drawn_graph）と複雑さマトリクスZ（drawn_neurons）をMatrixSamplerから取得\n",
        "            drawn_graph = graph_sampler()\n",
        "            drawn_neurons = neuron_sampler()\n",
        "            # (drawn_graph)のサイズは、torch.Size([nb_var, nb_var])。 出力値は0か1\n",
        "            # (drawn_neurons)のサイズは、torch.Size([nh, nb_var])。 出力値は0か1\n",
        "\n",
        "            # ノイズをリセットし、生成器Gで疑似データを生成\n",
        "            noise.normal_()\n",
        "            generated_variables = sam(data=batch, noise=noise,\n",
        "                                      adj_matrix=torch.cat(\n",
        "                                          [drawn_graph, noise_row], 0),\n",
        "                                      drawn_neurons=drawn_neurons)\n",
        "\n",
        "            # 識別器Dで判定\n",
        "            # 観測変数のリスト[]で、各torch.Size([data数, 1])が求まる\n",
        "            disc_vars_d = discriminator(generated_variables.detach(), batch)\n",
        "            # 観測変数のリスト[] で、各torch.Size([data数, 1])が求まる\n",
        "            disc_vars_g = discriminator(generated_variables, batch)\n",
        "            true_vars_disc = discriminator(batch)  # torch.Size([data数, 1])が求まる\n",
        "\n",
        "            # 損失関数の計算（DCGAN）\n",
        "            disc_loss = sum([criterion(gen, _false.expand_as(gen)) for gen in disc_vars_d]) / nb_var \\\n",
        "                + criterion(true_vars_disc, _true.expand_as(true_vars_disc))\n",
        "\n",
        "            gen_loss = sum([criterion(gen,\n",
        "                                      _true.expand_as(gen))\n",
        "                            for gen in disc_vars_g])\n",
        "\n",
        "            # 損失の計算（SAM論文のオリジナルのfgan）\n",
        "            #disc_loss = sum([torch.mean(torch.exp(gen - 1)) for gen in disc_vars_d]) / nb_var - torch.mean(true_vars_disc)\n",
        "            #gen_loss = -sum([torch.mean(torch.exp(gen - 1)) for gen in disc_vars_g])\n",
        "\n",
        "            # 識別器Dのバックプロパゲーションとパラメータの更新\n",
        "            if epoch < train_epochs:\n",
        "                disc_loss.backward()\n",
        "                d_optimizer.step()\n",
        "\n",
        "            # 生成器のGの損失の計算の残り（マトリクスの複雑さとDAGのNO TEAR）\n",
        "            struc_loss = lambda1 / batch_size*drawn_graph.sum()     # Mのloss\n",
        "            func_loss = lambda2 / batch_size*drawn_neurons.sum()   # Aのloss\n",
        "\n",
        "            regul_loss = struc_loss + func_loss\n",
        "\n",
        "            if epoch <= train_epochs * dagstart:\n",
        "                # epochが基準前のときは、DAGになるようにMへのNO TEARSの制限はかけない\n",
        "                loss = gen_loss + regul_loss\n",
        "\n",
        "            else:\n",
        "                # epochが基準後のときは、DAGになるようにNO TEARSの制限をかける\n",
        "                filters = graph_sampler.get_proba()  # マトリクスMの要素を取得（ただし、0,1ではなく、1の確率）\n",
        "                dag_constraint = notears_constr(filters*filters)  # NO TERARの計算\n",
        "\n",
        "                # 徐々に線形にDAGの正則を強くする\n",
        "                loss = gen_loss + regul_loss + \\\n",
        "                    ((epoch - train_epochs * dagstart) *\n",
        "                     dagpenalization_increase) * dag_constraint\n",
        "\n",
        "            if epoch >= train_epochs:\n",
        "                # testのepochの場合、結果を取得\n",
        "                output.add_(filters.data)\n",
        "                output_loss.add_(gen_loss.data)\n",
        "            else:\n",
        "                # trainのepochの場合、生成器Gのバックプロパゲーションと更新\n",
        "                # retain_graph=Trueにすることで、以降3つのstep()が実行できる\n",
        "                loss.backward(retain_graph=True)\n",
        "                g_optimizer.step()\n",
        "                graph_optimizer.step()\n",
        "                neuron_optimizer.step()\n",
        "\n",
        "            # 進捗の表示\n",
        "            if epoch % 50 == 0:\n",
        "                pbar.set_postfix(gen=gen_loss.item()/cols,\n",
        "                                 disc=disc_loss.item(),\n",
        "                                 regul_loss=regul_loss.item(),\n",
        "                                 tot=loss.item())\n",
        "\n",
        "    return output.cpu().numpy()/test_epochs, output_loss.cpu().numpy()/test_epochs/cols  # Mと損失を出力"
      ],
      "metadata": {
        "id": "W5rDqfIVbN7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUの使用確認：True or False\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "YmkZYYWcbY0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numpyの出力を小数点2桁に\n",
        "np.set_printoptions(precision=2, floatmode='fixed', suppress=True)\n",
        "\n",
        "# 因果探索の結果を格納するリスト\n",
        "m_list = []\n",
        "loss_list = []\n",
        "\n",
        "for i in range(5):\n",
        "    m, loss = run_SAM(in_data=df, lr_gen=0.01*0.5,\n",
        "                      lr_disc=0.01*0.5*2,\n",
        "                      #lambda1=0.01, lambda2=1e-05,\n",
        "                      lambda1=5.0*20, lambda2=0.005*20,\n",
        "                      hlayers=2,\n",
        "                      nh=200, dnh=200,\n",
        "                      train_epochs=10000,\n",
        "                      test_epochs=1000,\n",
        "                      device='cuda:0')\n",
        "\n",
        "    print(loss)\n",
        "    print(m)\n",
        "\n",
        "    m_list.append(m)\n",
        "    loss_list.append(loss)"
      ],
      "metadata": {
        "id": "I91R72s1cQ7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_m = sum(m_list) / len(m_list)\n",
        "print(final_m)"
      ],
      "metadata": {
        "id": "xQEIKB-6DTEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#閾値設定して1,0にする\n",
        "threshold = 0.6\n",
        "np.array([[1 if j > threshold else 0 for j in i] for i in final_m])"
      ],
      "metadata": {
        "id": "f2fOBEDPDUhc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}